---
phase: 01-foundation-infrastructure
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - apps/slack-backend/src/jobs/connection.ts
  - apps/slack-backend/src/jobs/queues.ts
  - apps/slack-backend/src/jobs/workers.ts
  - apps/slack-backend/src/jobs/index.ts
autonomous: true

must_haves:
  truths:
    - "Background jobs can be queued and processed asynchronously"
    - "Job queue has rate limiting configured per workspace"
    - "Failed jobs are retried with exponential backoff"
    - "Worker errors are handled without crashing the process"
  artifacts:
    - path: "apps/slack-backend/src/jobs/connection.ts"
      provides: "Redis connection for BullMQ"
      exports: ["redis"]
    - path: "apps/slack-backend/src/jobs/queues.ts"
      provides: "Job queue definitions"
      exports: ["aiResponseQueue"]
    - path: "apps/slack-backend/src/jobs/workers.ts"
      provides: "Job worker processors"
      exports: ["startWorkers", "stopWorkers"]
  key_links:
    - from: "apps/slack-backend/src/jobs/queues.ts"
      to: "apps/slack-backend/src/jobs/connection.ts"
      via: "Redis connection import"
      pattern: "import.*redis.*from.*connection"
    - from: "apps/slack-backend/src/jobs/workers.ts"
      to: "apps/slack-backend/src/jobs/queues.ts"
      via: "Queue name reference"
      pattern: "ai-responses"
---

<objective>
Set up BullMQ job queues and workers for asynchronous AI request processing, with rate limiting and retry logic to handle Slack's 3-second timeout constraint.

Purpose: AI generation takes 10-30 seconds, far exceeding Slack's 3-second webhook timeout. Background job processing (INFRA-03) allows immediate acknowledgment to Slack while processing continues asynchronously. Rate limiting (INFRA-05) prevents abuse and respects API limits.

Output: Working BullMQ queue with rate-limited worker, ready to process AI jobs in Phase 2.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation-infrastructure/01-RESEARCH.md
@.planning/phases/01-foundation-infrastructure/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Redis connection and job queue</name>
  <files>
    apps/slack-backend/src/jobs/connection.ts
    apps/slack-backend/src/jobs/queues.ts
    apps/slack-backend/src/jobs/types.ts
  </files>
  <action>
    Create apps/slack-backend/src/jobs/connection.ts:

    1. Import Redis from 'ioredis'
    2. Import { env } from '../env'
    3. Create and export redis connection:
       ```
       export const redis = new Redis({
         host: env.REDIS_HOST,
         port: env.REDIS_PORT,
         maxRetriesPerRequest: null, // Required for BullMQ
       });
       ```
    4. Add error handler: redis.on('error', (err) => log error but don't crash)
    5. Add ready handler: redis.on('ready', () => log 'Redis connected')

    Create apps/slack-backend/src/jobs/types.ts:

    Define job data interfaces:
    ```typescript
    export interface AIResponseJobData {
      workspaceId: string;
      userId: string;
      channelId: string;
      messageTs: string;
      triggerMessageText: string;
      contextMessages: Array<{
        userId: string;
        text: string;
        ts: string;
      }>;
      triggeredBy: 'mention' | 'reply' | 'thread' | 'message_action';
    }

    export interface AIResponseJobResult {
      suggestionId: string;
      suggestion: string;
      processingTimeMs: number;
    }
    ```

    Create apps/slack-backend/src/jobs/queues.ts:

    1. Import { Queue } from 'bullmq'
    2. Import { redis } from './connection'
    3. Import { AIResponseJobData } from './types'
    4. Create and export queue:
       ```typescript
       export const aiResponseQueue = new Queue<AIResponseJobData>('ai-responses', {
         connection: redis,
         defaultJobOptions: {
           attempts: 3,
           backoff: {
             type: 'exponential',
             delay: 2000, // Start at 2s, then 4s, then 8s
           },
           removeOnComplete: {
             count: 100, // Keep last 100 completed jobs
           },
           removeOnFail: {
             count: 500, // Keep last 500 failed jobs for debugging
           },
         },
       });
       ```
    5. Export helper function to add jobs:
       ```typescript
       export async function queueAIResponse(
         data: AIResponseJobData,
         options?: { priority?: number; delay?: number }
       ) {
         return aiResponseQueue.add('generate-suggestion', data, {
           priority: options?.priority,
           delay: options?.delay,
         });
       }
       ```
  </action>
  <verify>
    Run TypeScript compilation: npm run build
    Should compile without errors.

    With Redis running, test queue creation doesn't throw errors.
  </verify>
  <done>
    connection.ts creates Redis connection with error handling.
    types.ts defines AIResponseJobData and AIResponseJobResult interfaces.
    queues.ts creates ai-responses queue with retry and cleanup configuration.
    queueAIResponse helper function allows easy job creation.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create job worker with rate limiting</name>
  <files>
    apps/slack-backend/src/jobs/workers.ts
    apps/slack-backend/src/jobs/index.ts
    apps/slack-backend/src/index.ts
  </files>
  <action>
    Create apps/slack-backend/src/jobs/workers.ts:

    1. Import { Worker, Job } from 'bullmq'
    2. Import { redis } from './connection'
    3. Import { AIResponseJobData, AIResponseJobResult } from './types'
    4. Import pino for logging

    Create worker variable (initially null) and start/stop functions:

    ```typescript
    let aiResponseWorker: Worker<AIResponseJobData, AIResponseJobResult> | null = null;

    export async function startWorkers() {
      aiResponseWorker = new Worker<AIResponseJobData, AIResponseJobResult>(
        'ai-responses',
        async (job: Job<AIResponseJobData>) => {
          const startTime = Date.now();
          const { workspaceId, userId, channelId, triggerMessageText, contextMessages, triggeredBy } = job.data;

          logger.info({ jobId: job.id, workspaceId, triggeredBy }, 'Processing AI response job');

          // Update progress
          await job.updateProgress(10);

          // TODO: Phase 2 will implement actual AI generation
          // For now, return placeholder
          const suggestion = `[Placeholder] AI suggestion for: "${triggerMessageText.slice(0, 50)}..."`;

          await job.updateProgress(90);

          // TODO: Phase 2 will send ephemeral message back to Slack
          logger.info({ jobId: job.id, processingTimeMs: Date.now() - startTime }, 'Job completed');

          await job.updateProgress(100);

          return {
            suggestionId: crypto.randomUUID(),
            suggestion,
            processingTimeMs: Date.now() - startTime,
          };
        },
        {
          connection: redis,
          concurrency: 5, // Process 5 jobs in parallel per worker instance
          limiter: {
            max: 10, // Max 10 jobs per duration
            duration: 1000, // Per 1 second
          },
        }
      );

      // Critical: Attach error listeners to prevent crashes
      aiResponseWorker.on('error', (err) => {
        logger.error({ err }, 'Worker error');
      });

      aiResponseWorker.on('failed', (job, err) => {
        logger.error({ jobId: job?.id, err: err.message, attempts: job?.attemptsMade }, 'Job failed');
      });

      aiResponseWorker.on('completed', (job, result) => {
        logger.info({ jobId: job.id, processingTimeMs: result.processingTimeMs }, 'Job completed successfully');
      });

      aiResponseWorker.on('stalled', (jobId) => {
        logger.warn({ jobId }, 'Job stalled - will be retried');
      });

      logger.info('AI response worker started');
    }

    export async function stopWorkers() {
      if (aiResponseWorker) {
        await aiResponseWorker.close();
        aiResponseWorker = null;
        logger.info('AI response worker stopped');
      }
    }
    ```

    Create apps/slack-backend/src/jobs/index.ts:
    - Re-export queues: aiResponseQueue, queueAIResponse
    - Re-export workers: startWorkers, stopWorkers
    - Re-export types

    Update apps/slack-backend/src/index.ts:
    - Import { startWorkers, stopWorkers } from './jobs'
    - Call startWorkers() after app.start()
    - Add graceful shutdown:
      ```typescript
      const shutdown = async () => {
        logger.info('Shutting down...');
        await stopWorkers();
        await app.stop();
        process.exit(0);
      };

      process.on('SIGTERM', shutdown);
      process.on('SIGINT', shutdown);
      ```
  </action>
  <verify>
    Run: npm run build
    Should compile without TypeScript errors.

    Run app with Redis running:
    - Should log "AI response worker started"
    - Ctrl+C should trigger graceful shutdown
    - Should log "AI response worker stopped"
  </verify>
  <done>
    workers.ts creates Worker with rate limiting (10 jobs/second) and concurrency (5 parallel).
    Worker has error, failed, completed, stalled event handlers.
    index.ts exports queue and worker functions.
    Main index.ts starts workers on app start and stops them on shutdown.
    Graceful shutdown implemented with SIGTERM/SIGINT handlers.
  </done>
</task>

</tasks>

<verification>
1. apps/slack-backend/src/jobs/connection.ts creates Redis connection
2. apps/slack-backend/src/jobs/queues.ts creates ai-responses queue with retry config
3. apps/slack-backend/src/jobs/workers.ts creates worker with rate limiting
4. Worker has error handling that logs but doesn't crash process
5. Rate limiter configured: max 10 jobs per second
6. Retry configured: 3 attempts with exponential backoff (2s, 4s, 8s)
7. Graceful shutdown stops workers before process exit
8. All code compiles without TypeScript errors
</verification>

<success_criteria>
- Redis connection establishes (with Redis running)
- Job queue created with proper retry and cleanup settings
- Worker processes jobs with rate limiting
- Worker errors are logged, not thrown
- Graceful shutdown stops workers cleanly
- Ready for Phase 2 to add actual AI generation logic
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-infrastructure/01-03-SUMMARY.md`
</output>
