---
phase: 17-communication-pattern-insights
plan: 03
type: execute
wave: 2
depends_on: ["17-01"]
files_modified:
  - apps/slack-backend/src/services/trend-aggregator.ts
  - apps/slack-backend/src/jobs/types.ts
  - apps/slack-backend/src/jobs/queues.ts
  - apps/slack-backend/src/jobs/workers.ts
  - apps/slack-backend/src/services/index.ts
autonomous: true

must_haves:
  truths:
    - "aggregateDailyTrends function queries topic_classifications and escalation_alerts for a given date range per organization"
    - "Aggregated results are upserted into communication_trends table with JSONB columns"
    - "BullMQ queue 'trend-aggregation' exists with daily repeatable job at 3 AM UTC"
    - "Worker processes one org at a time with error isolation (one org failure doesn't stop others)"
    - "Channel hotspots are identified using minimum 10 message threshold"
  artifacts:
    - path: "apps/slack-backend/src/services/trend-aggregator.ts"
      provides: "aggregateDailyTrends function"
      exports: ["aggregateDailyTrends"]
    - path: "apps/slack-backend/src/jobs/queues.ts"
      provides: "trendAggregationQueue"
      contains: "trend-aggregation"
    - path: "apps/slack-backend/src/jobs/types.ts"
      provides: "TrendAggregationJobData and TrendAggregationJobResult types"
      contains: "TrendAggregationJobData"
  key_links:
    - from: "apps/slack-backend/src/jobs/workers.ts"
      to: "apps/slack-backend/src/services/trend-aggregator.ts"
      via: "worker processor calls aggregateDailyTrends"
      pattern: "aggregateDailyTrends"
    - from: "apps/slack-backend/src/services/trend-aggregator.ts"
      to: "packages/database/src/schema.ts"
      via: "queries topicClassifications, escalationAlerts, inserts communicationTrends"
      pattern: "communicationTrends"
---

<objective>
Create the trend aggregator service and BullMQ scheduled job for daily communication pattern aggregation.

Purpose: Precompute daily topic distribution, sentiment breakdown, escalation counts, and channel hotspots per organization to avoid expensive real-time aggregation on dashboard queries.

Output: trend-aggregator.ts service, BullMQ queue with daily 3 AM schedule, worker integration.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@apps/slack-backend/src/jobs/queues.ts
@apps/slack-backend/src/jobs/workers.ts
@apps/slack-backend/src/jobs/types.ts
@packages/database/src/schema.ts
@.planning/phases/17-communication-pattern-insights/17-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create trend aggregator service</name>
  <files>apps/slack-backend/src/services/trend-aggregator.ts, apps/slack-backend/src/services/index.ts</files>
  <action>
Create `apps/slack-backend/src/services/trend-aggregator.ts`:

1. Imports:
```typescript
import { db, organizations, topicClassifications, escalationAlerts, communicationTrends } from '@slack-speak/database';
import { sql, eq, and, gte, lt } from 'drizzle-orm';
import { logger } from '../utils/logger.js';
```

2. Export `aggregateDailyTrends` function:
```typescript
export async function aggregateDailyTrends(targetDate?: Date): Promise<{
  organizationsProcessed: number;
  trendsCreated: number;
  errors: number;
}>
```

The function:

a. Calculate the date range for aggregation:
   - If targetDate provided, use it. Otherwise use yesterday.
   - `dayStart` = targetDate at 00:00:00.000
   - `dayEnd` = dayStart + 1 day at 00:00:00.000

b. Fetch all organizations:
```typescript
const orgs = await db.select({ id: organizations.id }).from(organizations);
```

c. Process each org with error isolation (one org failure doesn't stop others):

For each org:

**Step 1: Topic distribution** — Query topic_classifications grouped by topic:
```sql
SELECT
  topic,
  COUNT(*) as count,
  AVG(confidence) as avg_confidence
FROM topic_classifications
WHERE organization_id = $orgId
  AND created_at >= $dayStart
  AND created_at < $dayEnd
GROUP BY topic
```
Convert to `Record<string, number>` for topicDistribution JSONB.

**Step 2: Sentiment distribution** — Query topic_classifications grouped by sentiment tone:
```sql
SELECT
  sentiment->>'tone' as tone,
  COUNT(*) as count
FROM topic_classifications
WHERE organization_id = $orgId
  AND created_at >= $dayStart
  AND created_at < $dayEnd
  AND sentiment IS NOT NULL
GROUP BY sentiment->>'tone'
```
Convert to `Record<string, number>` for sentimentDistribution JSONB.

**Step 3: Escalation counts** — Query escalation_alerts grouped by severity:
```sql
SELECT
  severity,
  COUNT(*) as count
FROM escalation_alerts
WHERE organization_id = $orgId
  AND created_at >= $dayStart
  AND created_at < $dayEnd
GROUP BY severity
```
Convert to `Record<string, number>` for escalationCounts JSONB.

**Step 4: Channel hotspots** — Identify channels with high complaint/escalation ratio:
```sql
SELECT
  channel_id,
  COUNT(*) FILTER (WHERE topic = 'complaint') as complaint_count,
  COUNT(*) FILTER (WHERE topic = 'escalation') as escalation_count,
  COUNT(*) as total_count
FROM topic_classifications
WHERE organization_id = $orgId
  AND created_at >= $dayStart
  AND created_at < $dayEnd
  AND channel_id IS NOT NULL
GROUP BY channel_id
HAVING COUNT(*) >= 10
```
For each row, calculate `hotspotRatio = (complaint_count + escalation_count) / total_count`. Only include channels where hotspotRatio > 0.3 (30% of messages are complaints or escalations). Store as `Array<{ channelId, complaintCount, escalationCount, totalCount, hotspotRatio }>`.

**Step 5: Compute totals** from topic distribution:
- totalClassifications = sum of all topic counts
- avgConfidence = weighted average from step 1

**Step 6: Upsert into communication_trends** using onConflictDoUpdate on the unique (organizationId, trendDate, trendPeriod) constraint:
```typescript
await db.insert(communicationTrends).values({
  organizationId: org.id,
  trendDate: dayStart,
  trendPeriod: 'daily',
  topicDistribution,
  sentimentDistribution,
  escalationCounts,
  channelHotspots,
  totalClassifications,
  avgConfidence,
}).onConflictDoUpdate({
  target: [communicationTrends.organizationId, communicationTrends.trendDate, communicationTrends.trendPeriod],
  set: {
    topicDistribution,
    sentimentDistribution,
    escalationCounts,
    channelHotspots,
    totalClassifications,
    avgConfidence,
  },
});
```

d. Return `{ organizationsProcessed, trendsCreated, errors }`.

3. Add to services/index.ts:
```typescript
// Trend aggregator - daily communication pattern aggregation
export { aggregateDailyTrends } from './trend-aggregator.js';
```

Use `db.execute(sql`...`)` for the raw SQL queries and manually parse the result rows (same pattern as response-time-analytics.ts). Convert PostgreSQL result rows to the appropriate types using Number() and String() casts.
  </action>
  <verify>Run `npx tsc --noEmit` in apps/slack-backend. Verify aggregateDailyTrends is exported from index.ts.</verify>
  <done>trend-aggregator.ts exists with aggregateDailyTrends function that processes all orgs, aggregates topic/sentiment/escalation/hotspot data, and upserts into communication_trends table.</done>
</task>

<task type="auto">
  <name>Task 2: Add BullMQ queue, job type, and worker for trend aggregation</name>
  <files>apps/slack-backend/src/jobs/types.ts, apps/slack-backend/src/jobs/queues.ts, apps/slack-backend/src/jobs/workers.ts</files>
  <action>
1. Add job types to `apps/slack-backend/src/jobs/types.ts`:
```typescript
export interface TrendAggregationJobData {
  triggeredBy: 'schedule' | 'manual';
  targetDate?: string; // ISO date string, optional override
}

export interface TrendAggregationJobResult {
  organizationsProcessed: number;
  trendsCreated: number;
  errors: number;
}
```

2. Add queue to `apps/slack-backend/src/jobs/queues.ts`:

Import the new type:
```typescript
import type { ..., TrendAggregationJobData } from './types.js';
```

Add queue:
```typescript
export const trendAggregationQueue = new Queue<TrendAggregationJobData>('trend-aggregation', {
  connection: redis,
  defaultJobOptions: {
    attempts: 3,
    backoff: {
      type: 'exponential',
      delay: 60000, // 1 min, 2 min, 4 min
    },
    removeOnComplete: { count: 50 },
    removeOnFail: { count: 100 },
  },
});
```

3. Add worker to `apps/slack-backend/src/jobs/workers.ts`:

Import new types:
```typescript
import type { ..., TrendAggregationJobData, TrendAggregationJobResult } from './types.js';
```

Import the service:
```typescript
import { aggregateDailyTrends } from '../services/trend-aggregator.js';
```

Add worker variable at top:
```typescript
let trendAggregationWorker: Worker<TrendAggregationJobData, TrendAggregationJobResult> | null = null;
```

In `startWorkers()`, add after the data retention worker:
```typescript
// Trend aggregation worker
trendAggregationWorker = new Worker<TrendAggregationJobData, TrendAggregationJobResult>(
  'trend-aggregation',
  async (job) => {
    logger.info({ jobId: job.id, triggeredBy: job.data.triggeredBy }, 'Processing trend aggregation job');
    const targetDate = job.data.targetDate ? new Date(job.data.targetDate) : undefined;
    const result = await aggregateDailyTrends(targetDate);
    logger.info({ jobId: job.id, ...result }, 'Trend aggregation completed');
    return result;
  },
  {
    connection: redis,
    concurrency: 1, // Only one aggregation at a time
  }
);

trendAggregationWorker.on('error', (err) => logger.error({ err }, 'Trend aggregation worker error'));
trendAggregationWorker.on('failed', (job, err) => logger.error({ jobId: job?.id, err: err.message }, 'Trend aggregation job failed'));
trendAggregationWorker.on('completed', (job, result) => logger.info({ jobId: job.id, ...result }, 'Trend aggregation job completed'));

logger.info('Trend aggregation worker started');
```

In `stopWorkers()`, add:
```typescript
if (trendAggregationWorker) {
  await trendAggregationWorker.close();
  trendAggregationWorker = null;
  logger.info('Trend aggregation worker stopped');
}
```

4. Set up the daily schedule. Find where other repeatable jobs are scheduled (likely in the app startup or a scheduler setup function). Add:
```typescript
// Schedule daily trend aggregation at 3 AM UTC
const existingJobs = await trendAggregationQueue.getRepeatableJobs();
for (const job of existingJobs) {
  if (job.name === 'aggregate-daily-trends') {
    await trendAggregationQueue.removeRepeatableByKey(job.key);
  }
}

await trendAggregationQueue.add(
  'aggregate-daily-trends',
  { triggeredBy: 'schedule' },
  {
    repeat: {
      pattern: '0 3 * * *', // Daily at 3 AM UTC
    },
  }
);
```

If there's a `setupSchedulers` or similar function in the codebase, add it there. Otherwise add it inline in `startWorkers()` after the worker setup.
  </action>
  <verify>Run `npx tsc --noEmit` in apps/slack-backend. Verify TrendAggregationJobData type exists. Verify trendAggregationQueue exists in queues.ts. Verify worker is started and stopped properly.</verify>
  <done>BullMQ trend-aggregation queue created with daily 3 AM schedule. Worker processes aggregation jobs with error isolation per org. Worker starts and stops cleanly with the rest of the workers.</done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes in apps/slack-backend
2. trend-aggregator.ts aggregates topics, sentiment, escalations, hotspots per org
3. BullMQ queue 'trend-aggregation' with daily 3 AM repeatable job
4. Worker processes jobs with concurrency 1 and proper error handling
5. Results upserted into communication_trends table
6. Worker lifecycle (start/stop) integrated properly
</verification>

<success_criteria>
Trend aggregation service and BullMQ job infrastructure in place. Daily job at 3 AM aggregates topic distribution, sentiment breakdown, escalation counts, and channel hotspots for each organization into the communication_trends table. Error isolation ensures one org failure doesn't block others.
</success_criteria>

<output>
After completion, create `.planning/phases/17-communication-pattern-insights/17-03-SUMMARY.md`
</output>
